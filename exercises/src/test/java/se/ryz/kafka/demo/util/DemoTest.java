package se.ryz.kafka.demo.util;

import org.apache.kafka.clients.admin.AdminClient;
import org.apache.kafka.clients.admin.ListTopicsOptions;
import org.apache.kafka.clients.admin.TopicListing;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.serialization.Serde;
import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.common.serialization.StringSerializer;
import org.apache.kafka.streams.KafkaStreams;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.StreamsConfig;
import org.apache.kafka.streams.errors.LogAndContinueExceptionHandler;
import org.apache.kafka.streams.kstream.KStream;
import org.junit.Test;

import java.time.Duration;
import java.util.Arrays;
import java.util.Collection;
import java.util.Properties;
import java.util.UUID;
import java.util.concurrent.ExecutionException;

import static se.ryz.kafka.demo.util.Common.KAFKA_BROKERS;

public class DemoTest {

    /**
     * Creates streams consumer configuration properties.
     * Sets up
     *      Kafka Brokers bootstrap
     *      Application ID
     *      Client ID
     *      Key and Value SerDe (both as String)
     *      LogAndContinueExeptionHandler
     * @return Consumer properties
     * @param applicationId Unique in the Kafka cluster. A new applicationId -> consume from beginning
     * @param clientId An ID used to trace application activity in Kafka logs
     */
    private Properties createStreamsConfig(String applicationId, String clientId) {
        if (applicationId == null) {
            applicationId = "autogenerated-" + UUID.randomUUID().toString();
        }
        final Properties streamsConfiguration = new Properties();
        // Give the Streams application a unique name.  The name must be unique in the Kafka cluster
        // against which the application is run. A new Application ID -> consume from beginning
        streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, applicationId);

        // Client ID:
        // An optional identifier of a Kafka consumer (in a consumer group) that is passed to a Kafka broker with every request.
        // The sole purpose of this is to be able to track the source of requests beyond just ip and port by allowing a logical application name to be included in Kafka logs and monitoring aggregates.
        streamsConfiguration.put(StreamsConfig.CLIENT_ID_CONFIG, clientId);

        // Where to find Kafka broker(s).
        streamsConfiguration.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, KAFKA_BROKERS);

        // Specify default (de)serializers for record keys and for record values.
        streamsConfiguration.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());
        streamsConfiguration.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());
        streamsConfiguration.put(StreamsConfig.DEFAULT_DESERIALIZATION_EXCEPTION_HANDLER_CLASS_CONFIG, LogAndContinueExceptionHandler.class);
        return streamsConfiguration;
    }

    /**
     * sets up a Processor producer
     *      Processor Client ID
     *      Kafka Bootstrap servers
     *      Key and Value SerDes (both as String)
     * @param clientId
     * @return
     */
    private Properties createProcessorProducerProperties(String clientId) {
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, KAFKA_BROKERS);
        // Used to trace activity in Kafka logs
        props.put(ProducerConfig.CLIENT_ID_CONFIG, clientId);
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        return props;
    }

    /*
    We do not receive an error, after 60 seconds we get a timeout, but flush does not throw ex
     */

    // -------------------------------------------------------------------------


    /**
     * Prints registered topics to console
     */
    @Test
    public void describeTopics() throws ExecutionException, InterruptedException {
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, KAFKA_BROKERS);
        AdminClient adminClient = AdminClient.create(props);
        adminClient.describeCluster();

        ListTopicsOptions listTopicsOptions = new ListTopicsOptions();
        listTopicsOptions.listInternal(false);
        Collection<TopicListing> topicListings = adminClient.listTopics(listTopicsOptions).listings().get();
        System.out.println("Number of registered topics (without internal): " + topicListings.size());
        topicListings.forEach(topicListing -> System.out.println("Topic name: '" + topicListing.name() + "', is internal: " + topicListing.isInternal()));


        System.out.println();

        listTopicsOptions = new ListTopicsOptions();
        listTopicsOptions.listInternal(true);
        topicListings = adminClient.listTopics(listTopicsOptions).listings().get();
        System.out.println("Number of registered topics (with internal): " + topicListings.size());
        topicListings.forEach(topicListing -> System.out.println("Topic name: '" + topicListing.name() + "', is internal: " + topicListing.isInternal()));
    }


    // -------------------------------------------------------------------------

    private TopicListing findTopicListingOrNull(String topicName) throws ExecutionException, InterruptedException {
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, KAFKA_BROKERS);
        AdminClient adminClient = AdminClient.create(props);
        adminClient.describeCluster();

        ListTopicsOptions listTopicsOptions = new ListTopicsOptions();
        listTopicsOptions.listInternal(true);
        Collection<TopicListing> topicListings = adminClient.listTopics(listTopicsOptions).listings().get();
        TopicListing foundTopicListing = topicListings
                .stream()
                .filter(topicListing ->
                        topicListing.name().equals(topicName)
                ).findAny()
                .orElse(null);
        adminClient.close();
        return foundTopicListing;
    }

    // -------------------------------------------------------------------------

    @Test
    public void doesTopicExist() throws ExecutionException, InterruptedException {
        String topicName = "_schemas";
        System.out.println("Does topic '" + topicName + "' exist? " + (findTopicListingOrNull(topicName) == null ? "No" : "Yes!"));
        topicName = "nonexistingtopic";
        System.out.println("Does topic '" + topicName + "' exist? " + (findTopicListingOrNull(topicName) == null ? "No" : "Yes!"));
    }

    // -------------------------------------------------------------------------

    @Test
    public void testAutoCreateProcessor() throws InterruptedException {
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, KAFKA_BROKERS);
        props.put("group.id", "dynamic-" + System.currentTimeMillis());
        //props.put("consumer.id", "autocreateconsumer3" + System.currentTimeMillis());
        props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");

        KafkaConsumer<String, String> consumer = new KafkaConsumer<String, String>(props);
        consumer.subscribe(Arrays.asList("everythingtopic"));

        for (; ; ) {
            System.out.println("Polling");
            ConsumerRecords<String, String> records = consumer.poll(Duration.ofHours(2));
            System.out.println("Received records: " + records.isEmpty() + ", num: " + records.count());
            //if (!records.isEmpty()) {
            for (ConsumerRecord record : records) {
                System.out.println("Key: " + record.key() + ", message: " + record.value());
            }
            //}
        }
    }

    /**
     * Tries to send messages to a non-existing Topic
     */
    @Test
    public void testAutoCreateTopicBySendingMessagesToNonExistentTopic() throws InterruptedException {
        Properties producerProperties = createProcessorProducerProperties("autoCreateProducer");
        KafkaProducer<String, String> producer = new KafkaProducer<>(producerProperties);
        ProducerRecord<String, String> record = new ProducerRecord<>("nonexistingtopic", "msgkey", "msgvalue");
        producer.send(record);
        producer.flush();
        System.out.println("Sent record");
        producer.close();
    }
    @Test
    public void runE() throws InterruptedException {

       /* Properties streamsConfiguration = new Properties();
        streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, "hello-world-stream" + System.currentTimeMillis());
        streamsConfiguration.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, KAFKA_BROKERS);
        //streamsConfiguration.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, schemaRegistry);
        streamsConfiguration.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        streamsConfiguration.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        //streamsConfiguration.put(StreamsConfig.DEFAULT_DESERIALIZATION_EXCEPTION_HANDLER_CLASS_CONFIG, LogAndIgnoreExceptionHandler.class.getName());


*/
        Properties streamsConfiguration = createStreamsConfig("testApplication", "client");

        final StreamsBuilder builder = new StreamsBuilder();
        final KStream<String, String> textLines = builder.stream("everything-topic");
        textLines.foreach((key, value) -> System.out.println("Key: " + key + ", value: " + value));
        final KafkaStreams streams = new KafkaStreams(builder.build(), streamsConfiguration);

        streams.start();

        //Runtime.getRuntime().addShutdownHook(new Thread(streams::close));
        for (; ; ) {
            Thread.sleep(1000);
        }

    }

    @Test
    public void testSimpleConsumer() throws InterruptedException {

        Properties streamsConfiguration = createStreamsConfig(null, "SimpleConsumer");
        final Serde<String> stringSerde = Serdes.String();
        final Serde<Long> longSerde = Serdes.Long();

        // In the subsequent lines we define the processing topology of the Streams application.
        final StreamsBuilder builder = new StreamsBuilder();

        // Construct a `KStream` from the input topic "streams-plaintext-input", where message values
        // represent lines of text (for the sake of this example, we ignore whatever may be stored
        // in the message keys).
        //
        // Note: We could also just call `builder.stream("streams-plaintext-input")` if we wanted to leverage
        // the default serdes specified in the Streams configuration above, because these defaults
        // match what's in the actual topic.  However we explicitly set the deserializers in the
        // call to `stream()` below in order to show how that's done, too.
        final KStream<String, String> textLines = builder.stream("everythingtopic");
        final KafkaStreams streams = new KafkaStreams(builder.build(), streamsConfiguration);

        textLines.foreach((key, value) -> System.out.println("Key: " + key + ", value: " + value));

        streams.start();
        for (; ; ) {
            Thread.sleep(1000);
        }
        //Runtime.getRuntime().addShutdownHook(new Thread(streams::close));

    }

    @Test
    public void viewTopicConfig() {
        final String bootstrapServers = KAFKA_BROKERS;
        final Properties streamsConfiguration = new Properties();
        // Give the Streams application a unique name.  The name must be unique in the Kafka cluster
        // against which the application is run.
        streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, "wordcount-lambda-example");
        streamsConfiguration.put(StreamsConfig.CLIENT_ID_CONFIG, "wordcount-lambda-example-client");
        // Where to find Kafka broker(s).
        streamsConfiguration.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        // Specify default (de)serializers for record keys and for record values.
        streamsConfiguration.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());
        streamsConfiguration.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());
        // Records should be flushed every 10 seconds. This is less than the default
        // in order to keep this example interactive.
        streamsConfiguration.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 10 * 1000);
        // For illustrative purposes we disable record caches
        streamsConfiguration.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);

        // Set up serializers and deserializers, which we will use for overriding the default serdes
        // specified above.
        final Serde<String> stringSerde = Serdes.String();
        final Serde<Long> longSerde = Serdes.Long();

        // In the subsequent lines we define the processing topology of the Streams application.
        final StreamsBuilder builder = new StreamsBuilder();

        // Construct a `KStream` from the input topic "streams-plaintext-input", where message values
        // represent lines of text (for the sake of this example, we ignore whatever may be stored
        // in the message keys).
        //
        // Note: We could also just call `builder.stream("streams-plaintext-input")` if we wanted to leverage
        // the default serdes specified in the Streams configuration above, because these defaults
        // match what's in the actual topic.  However we explicitly set the deserializers in the
        // call to `stream()` below in order to show how that's done, too.
        final KStream<String, String> textLines = builder.stream("streams-plaintext-input");
        final KafkaStreams streams = new KafkaStreams(builder.build(), streamsConfiguration);
        streams.start();
        Runtime.getRuntime().addShutdownHook(new Thread(streams::close));

    }




}
